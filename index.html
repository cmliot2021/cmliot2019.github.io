<!DOCTYPE html>
<html lang="en">
  <head>
    <title>CML-IOT 2019</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link href="https://fonts.googleapis.com/css?family=Amatic+SC:400,700|Work+Sans:300,400,700" rel="stylesheet">
    <link rel="stylesheet" href="fonts/icomoon/style.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/bootstrap-datepicker.css">
    <link rel="stylesheet" href="css/animate.css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/mediaelementplayer.min.css">
    
    
    
    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
  
    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/style.css">
    
  </head>
  <body>
  
  <div class="site-wrap">

    <div class="site-mobile-menu">
      <div class="site-mobile-menu-header">
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div> <!-- .site-mobile-menu -->
    
    
    <div class="site-navbar-wrap js-site-navbar bg-white">
      
      <div class="container">
        <div class="site-navbar bg-light">
          <div class="py-1">
            <div class="row align-items-center">
              <div class="col-8">
                <h2 class="mb-0 site-logo"><a href="index.html">CML-IOT 2019</a></h2>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  
    
    
      
    <div class="site-blocks-cover overlay" style="background-image: url(images/london.jpg);" data-aos="fade" data-stellar-background-ratio="0.5">
      <div class="container">
        <div class="row align-items-center justify-content-center">
          <div class="col-md-10 text-center" data-aos="fade">
            <font size="10" color="white">Continual and Multimodal Learning for Internet of Things</font>
            <p class="mb-5">September 9, 2019 &bullet; London, UK</p>
		  <p class="mb-6">A UbiComp 2019 Workshop</p>
          </div>
        </div>
      </div>
    </div>  

<a name="more"></a>

      

    <div class="site-section">
      <div class="container">
	      
        <div class="row">
          <div class="col-md-12 mx-auto text-left section-heading">
             <h3 class="mb-5 text-uppercase">About CML-IOT </h3> 
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
	<p>Internet of Things (IoT) provides streaming, large-amount, and multimodal sensing data over time. The statistical properties of these data are often significantly different by sensing modalities and temporal traits, which are hardly captured by conventional learning methods. Continual and multimodal learning allows integration, adaptation and generalization of the knowledge learnt from previous experiential data collected with heterogeneity to new situations. Therefore, continual and multimodal learning is an important step to improve the estimation, utilization, and security of real-world data from IoT devices. </p>
	<br />
    <br />
          </div>




          <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Call for Papers </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <p>
		    This workshop aims to explore the intersection and combination of continual machine learning and multimodal modeling with applications in Internet of Things. The workshop welcomes works addressing these issues in different applications and domains, such as human-centric sensing, smart cities, health and wellness, privacy and security, etc. We aim at bringing together researchers from different areas to establish a multidisciplinary community and share the latest research. 
		    </p>
	<p>We focus on the novel learning methods that can be applied on streaming multimodal data:</p>
	<li>online learning </li>
	<li>transfer learning </li>
<li>few-shot learning </li>
<li>multi-task learning </li>
<li>reinforcement learning </li>
<li>learning without forgetting </li>
<li>individual and/or institutional privacy </li>
<li>balance on-device and off-device learning </li>
<li>manage high volume data flow</li>

		  	
	 <br />
		  
	<p>We also welcome continual learning methods that target:  </p>
<li>data distribution changed caused by the fast-changing dynamic physical environment</li>
<li>missing, imbalanced, or noisy data under multimodal sensing scenarios</li>
		  
	<br />
		  
<p> Novel applications or interfaces on streaming multimodal data are also related topics. </p>

		  <br />
<p>As examples, the data modalities include but not limited to: WiFi, GPS, RFID, vibration, accelerometer, pressure, temperature, humidity, biochemistry, image, video, audio, speech, natural language, virtual reality, etc.</p>
    <br />
    <br />	  
          </div>



           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Important Dates </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
	<li>Submission deadline: June 29, 2019 </li>
	<li>Notification of acceptance: July 8, 2019 </li>
	<li>Deadline for camera ready version: July 12, 2019</li>
	<li>Workshop: September 9, 2019</li>
	<p> </p>
	<p><a href="https://new.precisionconference.com" class="btn btn-primary px-4 py-2">Submit Now</a></p>
	<br />
    <br />
          </div>

           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Submission Guidelines </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <p>Please submit papers using the <a href="https://www.acm.org/publications/proceedings-template">ACM SIGCHI portrait template</a>. We invite papers of varying length from 2 to 6 pages, plus additional pages for the reference; i.e., the reference page(s) are not counted to the limit of 6 pages. Accepted papers will be included in the ACM Digital Library and supplemental proceedings of the conference. Reviews are not double-blind, and author names and affiliations should be listed.</p>
	<br />
    <br />     
          </div>


 <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Keynote </h3>
            </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">

<h5 class="mb-5 text">The Deep (Learning) Transformation of Mobile and Embedded Computing, Speaker: Nic Lane, University of Oxford & Samsung AI Center, Cambridge</h5>

<div class="row justify-content-center">
          <div class="col-12">

<p>Abstract: Mobile and embedded devices increasingly rely on deep neural networks to understand the world -- a formerly impossible feat that would have overwhelmed their system resources just a few years ago. The age of on-device artificial intelligence is upon us; but incredibly, these dramatic changes are just the beginning. Looking ahead, mobile machine learning will extend beyond just classifying categories and perceptual tasks, to roles that alter how every part of the systems stack of smart devices function. This evolutionary step in constrained-resource computing will finally produce devices that meet our expectations in how they can learn, reason and react to the real-world. In this talk, I will briefly discuss the initial breakthroughs that allowed us to reach this point, and outline the next set of open problems we must overcome to bring about this next deep transformation of mobile and embedded computing. </p>
	
<p>Speaker Bio: Nic Lane is an Associate Professor in the Computer Science Department at the University of Oxford and Program Director (AI Systems) at the recently announced Samsung AI Center at Cambridge. Before joining Oxford, he held dual appointments at University College London (UCL) and Nokia Bell Labs; at Nokia, as a Principal Scientist, Nic founded and led DeepX – an embedded focused deep learning unit at the Cambridge location. Of late his research has specialized in the study of efficient machine learning under computational constraints, and over the last three years he has pioneered a range of embedded and mobile forms of deep learning. This work formed the basis for his 2017 Google Faculty Award in machine learning. More generally, Nic’s research interests revolve around the modelling and systems challenges that arise when computers collect and reason over various types of complex real-world people-centric data. Nic has received multiple best paper awards, including ACM/IEEE IPSN 2017 and two from ACM UbiComp (2012 and 2015). In 2018 and 2019, he (and his co-authors) received the ACM SenSys Test-of-Time award and ACM SIGMOBILE Test-of-Time award for pioneering research, performed during his PhD thesis, that devised machine learning algorithms used today on devices like smartphones. This year Nic served as the PC-chair of ACM MobiSys 2019, a role he has performed also for ACM HotMobile and ACM SenSys in the past. Prior to moving to England, Nic spent 4-years at Microsoft Research based in Beijing as a Lead Researcher. He received his PhD from Dartmouth College in 2011. </p>
<br>
<br>
  </div>
        </div>
         </div>
		
     <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Organizers</h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <h5 class="mb-5 text">Workshop Chairs (Feel free to contact us by cmliot2019@gmail.com, if you have any questions.) </h5>
            <li>Tong Yu (Samsung Research America)</li>
            <li>Shijia Pan (Carnegie Mellon University)</li>
            <li>Susu Xu (Carnegie Mellon University)</li>
            <li>Yilin Shen (Samsung Research America)</li>
            <li>Botao Hao (Purdue Unversity)</li>
            <br />
            <br />
            <h5 class="mb-5 text">Advising Committee</h5>
            <li>Pei Zhang (Carnegie Mellon University)</li>
            <li>Hae Young Noh (Carnegie Mellon University)</li>
            <li>Jennifer Healey (Adobe Research)</li>
            <li>Thomas Ploetz (Georgia Institute of Technology)</li>
            <li>Branislav Kveton (Google Research)</li>
            <li>Hongxia Jin (Samsung Research America)</li>
            <br />
            <br />
		  
	    <h5 class="mb-5 text">Technical Program Committee</h5>
            <li>Sheng Li (University of Georgia)</li>
            <li>Yuan Tian (University of Virginia)</li>
   <li>Chenren Xu (Peking University) </li>
   <li>Jun Han (National University of Singapore)</li>
   <li>Shuai Li (Chinese University of Hong Kong) </li>
   <li>Xiaoxuan Lu (University of Oxford)</li>
   <li>Dezhi Hong (University of California San Diego) </li>
		  <li> Mostafa Mirshekari (Carnegie Mellon University)</li>
            <li>Ming Zeng (Carnegie Mellon University)</li>
            <li>Ruiyi Zhang (Duke University)</li>
            <li>Charles Chen (Ohio University)</li>
   <li>Kaifei Chen (Waymo)</li>
            <li>Avik Ray (Samsung Research America)</li>
            <li>Yue Deng (Samsung Research America)</li>
            <li>Xiao Wang (Facebook)</li>
            <li>Bing Liu (Facebook AI)</li>  
		  <br />
    <br />
	</div>

    
    <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Agenda </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <h5 class="mb-5 text">Registration/Doors Open (9:30) </h5>
	    <h5 class="mb-5 text">Welcome! (10:00 - 10:15), Speaker: Shijia Pan, University of California Merced</h5>  
	 	
	    <h5 class="mb-5 text">Keynote (10:15 - 11:00),  Speaker: Nic Lane, University of Oxford & Samsung AI Center, Cambridge </h5>  
	    
	    <h5 class="mb-6 text">Session 1: Adaptation in Mutimodal Learning (11:00 - 12:30), Chair: Botao Hao </h6> 
	    <li>Unsupervised Domain Adaptation for Robust Sensory Systems, Akhil Mathur, Anton Isopoussu, Nadia Bianchi-Berthouze, Nicholas D Lane, Fahim Kawsar</li> 

	    <li>iSCAN: Automatic Speaker Adaptation via Iterative Cross-modality Association, Yuanbo Xiangli, Chris Xiaoxuan Lu, Peijun Zhao, Changhao Chen, Andrew Markham</li> 

	    <li>AutoTag: Visual Domain Adaptation for Autonomous Retail Stores through Multi-Modal Sensing, Carlos Ruiz, Joao Falcao, Pei Zhang</li> 

	    <li>AttriNet: Learning Mid-Level Features for Human Activity Recognition with Deep Belief Networks, Harideep Nair, Shunwen Tan, Ming Zeng, Ole Mengshoel, John Paul Shen</li> 

	    
	    <br>
	    <h5 class="mb-5 text">Lunch (12:30 - 14:00) </h5>
		  
        <h5 class="mb-6 text">Session 2: Mutimodal Mobile Sensing (14:00 - 15:30), Chair: Zhihan Fang </h5>
        <li>Inferring Fine-Grained Air Pollution Map via a Spatiotemporal Super-Resolution Scheme, Ning Liu, Rui Ma, Yue Wang, Lin Zhang</li> 

	    <li>mLung++: Automated Characterization of Abnormal Lung Sounds in Pulmonary Patients using Multimodal Mobile Sensors, Soujanya Chatterjee, Md Mahbubur Rahman, Ebrahim Nemanti, Viswam Nathan, Korosh Vatanparvar, Jilong Kuang</li> 

	    <li>PRECEPT: Occupancy Presence Prediction Inside A Commercial Building, Anooshmita Das, Mikkel Baun Kjærgaard</li> 

	    <li>Towards a Taxonomy of Interactive Continual and Multimodal Learning for the Internet of Things, Agnes Tegen, Paul Davidsson, Jan A Persson</li> 

	    
	    <br>
	
		<h5 class="mb-5 text">Coffee Pause (15:30 - 16:10)</h5>  

	    <h5 class="mb-6 text">Session 3: Vision and Language (16:10 - 17:20), Chair: Carlos Ruiz </h5> 
	    <li>Audio-Visual TED Corpus: Enhancing the TED-LIUM Corpus with Facial Information, Contextual Text and Object Recognition, Guan-Lin Chao, Chih Chi Hu, Bing Liu, John Paul Shen, Ian Lane</li> 

	    <li>Apply Event Extraction Techniques to the Judicial Field, Chuanyi Li, Yu Sheng, Jidong Ge, Bin Luo</li> 

	    <li>Neural Caption Generation over Figures, Charles Chen</li> 

	    
	    



	    <br>

	    <h5 class="mb-5 text">Registration/Doors Close (17:50) </h5>  
            
            <br>
	    Note: For each paper presentation, there are 15 minutes for presentation and 5 minutes for Q&A.
		
          </div>
        </div>
      </div>
    </div>

    
    

          
          
        <div class="row pt-5 mt-5 text-center">
          <div class="col-md-12">
            <p>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            Copyright &copy; <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>document.write(new Date().getFullYear());</script> All Rights Reserved | This template is made with <i class="icon-heart text-primary" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank" >Colorlib</a>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </p>
          </div>
          
        </div>
      </div>
    </footer>
  </div>

  <script src="js/jquery-3.3.1.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/jquery-ui.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/jquery.countdown.min.js"></script>
  <script src="js/jquery.magnific-popup.min.js"></script>
  <script src="js/bootstrap-datepicker.min.js"></script>
  <script src="js/aos.js"></script>

  
  <script src="js/mediaelement-and-player.min.js"></script>

  <script src="js/main.js"></script>
    

  <script>
      document.addEventListener('DOMContentLoaded', function() {
                var mediaElements = document.querySelectorAll('video, audio'), total = mediaElements.length;

                for (var i = 0; i < total; i++) {
                    new MediaElementPlayer(mediaElements[i], {
                        pluginPath: 'https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/',
                        shimScriptAccess: 'always',
                        success: function () {
                            var target = document.body.querySelectorAll('.player'), targetTotal = target.length;
                            for (var j = 0; j < targetTotal; j++) {
                                target[j].style.visibility = 'visible';
                            }
                  }
                });
                }
            });
    </script>

  </body>
</html>
